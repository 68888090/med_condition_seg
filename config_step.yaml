# config.yaml

training:
  logdir: "test"                 # directory to save the tensorboard logs
  epochs: 100000                    # number of training epochs
  num_steps: 100000              # number of training iterations
  eval_num: 100                  # evaluation frequency
  warmup_steps: 500              # warmup steps
  batch_size: 12                 # number of batch size
  sw_batch_size: 1             # number of sliding window batch size
  resume: null                   # resume training (path to checkpoint)
  loss_type: "SSL"               # loss type
  log_num: 10
  n_folds: 10
  patience: 1000
  main_loss_weight: 1.0
  aux_loss_weight: 0.2
  reg_loss_weight: 0.0
  cls_loss_weight: 0.4

  val_target: "main"  # “main / "aux"

model:
  in_channels: 1                 # number of input channels
  feature_size: 48               # embedding size
  dropout_rate: 0.2         # drop path rate
  spatial_dims: 3                # spatial dimension of input data
  use_checkpoint: false          # use gradient checkpointing to save memory
  fusion_mode: "prior_guide"     # "prior_guide" / "simple_gate" / "attention"
  # 关于attention上面有数值不稳定的稳定，导致batch error

data:
  # Intensity Scaling
  a_min: -1000.0
  a_max: 1000.0
  b_min: 0.0
  b_max: 1.0
  
  # Spacing
  space_x: 1.0
  space_y: 1.0
  space_z: 1.0
  
  # ROI Size
  roi_x: 64
  roi_y: 64
  roi_z: 64
  
  # Caching (Mutual exclusive usually, but listed here)
  smartcache_dataset: false      # use monai smartcache Dataset
  cache_dataset: false           # use monai cache Dataset
  jsonlist: "/home/lhr/dataset/Union_for_lx_Rigid_lung_mask_Crop_64mm_Rigid/train_dataloader.json"

optimization:
  stage1_steps: 40000
  stage1_lr: 4.0e-4
  stage2_lr: 5.0e-5
  lr: 4.0e-4                     # learning rate
  decay: 0.1                     # decay rate
  momentum: 0.9                  # momentum
  lrdecay: true                 # enable learning rate decay
  max_grad_norm: 1.0             # maximum gradient norm
  grad_clip: true               # gradient clip
  opt: "adamw"                   # optimization algorithm
  lr_schedule: "warmup_cosine"

system:
  dist_url: "env://"             # url used to set up distributed training
  local_rank: 0                  # local rank
  noamp: false                    # Use Automatic Mixed Precision (注意：我将 noamp 改为了正向的 amp)
  device: "cuda:0"               # device to use for training
  distributed: false              # enable distributed training
  workers: 8                  # number of workers for data loading
  output_dir: '/home/lhr/dataset/checkpoints/swin-unetr'

experiment:
  project: "UnionData"
  name: "step_1"
  mode: "pure_seg"  #pure_seg \  train_multimodal